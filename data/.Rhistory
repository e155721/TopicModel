lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))
topics <- tidy(lda, matrix = "beta")
topics
View(topics)
DTM
View(DTM)
DTM[["dimnames"]][["Terms"]]
VI
View(LDA)
lds
lda
?tidy
?tidytext
?tidy
detach("package:topicmodels", unload=TRUE)
library("topicmodels", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
detach("package:topicmodels", unload=TRUE)
topics <- tidy(lda, matrix = "beta")
library("topicmodels", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
detach("package:tidytext", unload=TRUE)
topics <- tidy(lda, matrix = "beta")
library("tidytext", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
?tidy
topics <- tidy(lda, matrix = "gamma")
View(topics)
# read in the libraries we're going to use
library(tidyverse) # general utility & workflow functions
library(tidytext) # tidy implimentation of NLP methods
library(topicmodels) # for LDA topic modelling
library(tm) # general text mining functions, making document term matrixes
library(SnowballC) # for stemming
# function to get & plot the most informative terms by a specificed number
# of topics, using LDA
top_terms_by_topic_LDA <- function(input_text, # should be a columm from a dataframe
plot = T, # return a plot? TRUE by defult
number_of_topics = 4) # number of topics (4 by default)
{
# create a corpus (type of object expected by tm) and document term matrix
Corpus <- Corpus(VectorSource(input_text)) # make a corpus object
DTM <- DocumentTermMatrix(Corpus) # get the count of words/document
# remove any empty rows in our document term matrix (if there are any
# we'll get an error when we try to run our LDA)
unique_indexes <- unique(DTM$i) # get the index of each unique value
DTM <- DTM[unique_indexes,] # get a subset of only those indexes
# preform LDA & get the words/topic in a tidy text format
lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))
topics <- tidy(lda, matrix = "beta")
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
# if the user asks for a plot (TRUE by default)
if(plot == T){
# plot the top ten terms for each topic in order
top_terms %>% # take the top terms
mutate(term = reorder(term, beta)) %>% # sort terms by beta value
ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
geom_col(show.legend = FALSE) + # as a bar plot
facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
labs(x = NULL, y = "Beta") + # no x label, change y label
coord_flip() # turn bars sideways
}else{
# if the user does not request a plot
# return a list of sorted terms instead
##return(top_terms)
return(lda)
}
}
# read in our data
##reviews <- read_csv("deceptive-opinion.csv")
##input_text <- reviews$text
##reviews <- read_csv("pubmed_result.csv")
##input_text <- reviews$Description
clean <- function(input_text) {
# create a document term matrix to clean
reviewsCorpus <- Corpus(VectorSource(input_text))
reviewsDTM <- DocumentTermMatrix(reviewsCorpus)
# convert the document term matrix to a tidytext corpus
reviewsDTM_tidy <- tidy(reviewsDTM)
# I'm going to add my own custom stop words that I don't think will be
# very informative in hotel reviews
custom_stop_words <- tibble(word = c("hotel", "room"))
# remove stopwords
reviewsDTM_tidy_cleaned <- reviewsDTM_tidy %>% # take our tidy dtm and...
anti_join(stop_words, by = c("term" = "word")) %>% # remove English stopwords and...
anti_join(custom_stop_words, by = c("term" = "word")) # remove my custom stopwords
# stem the words (e.g. convert each word to its stem, where applicable)
reviewsDTM_tidy_cleaned <- reviewsDTM_tidy_cleaned %>%
mutate(stem = wordStem(term))
input_text <- reviewsDTM_tidy_cleaned$stem
return(input_text)
}
readFile <- function(dir)
{
fileVec <- list.files(file.path(getwd(),dir),
pattern=".txt",
full.names=T)
len <- length(fileVec)
file <- list(1:len)
for (i in 1:len) {
file[i] <- read_file(paste(fileVec[i]))
}
return(file)
}
input_text <- readFile("wiki")
input_text <- clean(input_text)
#lda <- top_terms_by_topic_LDA(input_text, number_of_topics = 3, plot=F)
top_terms_by_topic_LDA(input_text, number_of_topics = 2)
# read in the libraries we're going to use
library(tidyverse) # general utility & workflow functions
library(tidytext) # tidy implimentation of NLP methods
library(topicmodels) # for LDA topic modelling
library(tm) # general text mining functions, making document term matrixes
library(SnowballC) # for stemming
###
number_of_topics = 3
# create a corpus (type of object expected by tm) and document term matrix
Corpus <- Corpus(VectorSource(input_text)) # make a corpus object
DTM <- DocumentTermMatrix(Corpus) # get the count of words/document
# remove any empty rows in our document term matrix (if there are any
# we'll get an error when we try to run our LDA)
unique_indexes <- unique(DTM$i) # get the index of each unique value
DTM <- DTM[unique_indexes,] # get a subset of only those indexes
# preform LDA & get the words/topic in a tidy text format
lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))
topics <- tidy(lda, matrix = "beta")
View(topics)
?arrange
topics <- tidy(lda, matrix = "gamma")
View(topics)
# read in the libraries we're going to use
library(tidyverse) # general utility & workflow functions
library(tidytext) # tidy implimentation of NLP methods
library(topicmodels) # for LDA topic modelling
library(tm) # general text mining functions, making document term matrixes
library(SnowballC) # for stemming
# function to get & plot the most informative terms by a specificed number
# of topics, using LDA
top_terms_by_topic_LDA <- function(input_text, # should be a columm from a dataframe
plot = T, # return a plot? TRUE by defult
number_of_topics = 4) # number of topics (4 by default)
{
# create a corpus (type of object expected by tm) and document term matrix
Corpus <- Corpus(VectorSource(input_text)) # make a corpus object
DTM <- DocumentTermMatrix(Corpus) # get the count of words/document
# remove any empty rows in our document term matrix (if there are any
# we'll get an error when we try to run our LDA)
unique_indexes <- unique(DTM$i) # get the index of each unique value
DTM <- DTM[unique_indexes,] # get a subset of only those indexes
# preform LDA & get the words/topic in a tidy text format
lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))
topics <- tidy(lda, matrix = "beta")
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
# if the user asks for a plot (TRUE by default)
if(plot == T){
# plot the top ten terms for each topic in order
top_terms %>% # take the top terms
mutate(term = reorder(term, beta)) %>% # sort terms by beta value
ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
geom_col(show.legend = FALSE) + # as a bar plot
facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
labs(x = NULL, y = "Beta") + # no x label, change y label
coord_flip() # turn bars sideways
}else{
# if the user does not request a plot
# return a list of sorted terms instead
##return(top_terms)
return(lda)
}
}
reviews <- read_csv("deceptive-opinion.csv")
input_text <- reviews$text
input_text <- clean(input_text)
input_text
#lda <- top_terms_by_topic_LDA(input_text, number_of_topics = 3, plot=F)
top_terms_by_topic_LDA(input_text, number_of_topics = 2)
View(DTM)
View(Corpus)
?Corpus
DTM[["dimnames"]][["Docs"]]
?getOption
getOption(10000)
DTM[["dimnames"]][["Docs"]]
getOption(max.pring = 10000)
getOption(max.print = 10000)
DTM[["dimnames"]][["Terms"]]
DTM[["nrow"]]
DTM[["dimnames"]][["Terms"]]
DTM[["dimnames"]][["Docs"]]
DTM[["dimnames"]][["Terms"]]
View(Corpus)
# 初期設定
maxnum <- 10000
maxcat <- 1000
grpnum <- 4
maxdev <- maxcat / grpnum
set.seed(4756)
# 各データ内のデータ数を指数分布に従って発生させる
cnumf <- function(){
set.seed(4756)
round(rexp(maxnum, rate=0.3)+0.5)
}
cnum <- cnumf()
# グループを一様分布で割り当てる
gnumf <- function(){
set.seed(4756)
round(runif(maxnum, min=0.5, max=grpnum + 0.49999))
}
gnum <- gnumf()
# 各データに対して指定個数の乱数を割り当てる
# 割り当てる乱数はN((100*groupNum)+100, 50)の正規分布に従う
library(foreach)
library(Matrix)
# データ作成のための乱数のseedを一様乱数で設定する
seedsVecF <- function(){
set.seed(4756)
round(runif(maxnum)*1000000)
}
seedsVec <- seedsVecF()
cnormf <- function(i){
set.seed(seedsVec[i])
normNum <- rnorm(cnum[i],
mean = (gnum[i]*100)+100,
sd =50)
normNum <- unique(abs(round(normNum)))
return(normNum)
}
cnorm <- foreach(i=1:maxnum) %do% cnormf(i)
# データ確認のためヒストグラムを描く
g1 <- unlist(cnorm[gnum==1])
g2 <- unlist(cnorm[gnum==2])
g3 <- unlist(cnorm[gnum==3])
g4 <- unlist(cnorm[gnum==4])
hist(g1, col="#ff634740", breaks=100, xlim=c(0,700))
hist(g2, col="#ffd70040", breaks=100, xlim=c(0,700), add=T)
hist(g3, col="#228b2240", breaks=100, xlim=c(0,700), add=T)
hist(g4, col="#4169e140", breaks=100, xlim=c(0,700), add=T)
# 文字列ベクトルに変換する
cVec <- sapply(seq(1,length(cnorm)),function(x){
paste(cnorm[[x]], collapse=" ")
})
library(text2vec)
# document term matrixを作る
tokenizer <- itoken(cVec, ids=seq(1,length(cVec)))
vocab <- create_vocabulary(tokenizer)
vectorizer <- vocab_vectorizer(vocab)
dtm <- create_dtm(tokenizer, vectorizer)
library(LDAvis)
# topic model LDA
ldaModel <- LatentDirichletAllocation$new(n_topics=4, vocabulary=vocab)
ldaRes <- ldaModel$fit(dtm, n_iter=1000, check_convergence_every_n = 1)
#ldaRes$plot()
# 各トピックにおける単語の出現頻度を集計する
wordList <- ldaRes$get_word_vectors()
order <- order(as.numeric(rownames(wordList)))
wordList <- wordList[order,]
# プロット
plot("number","frequency",type="n",
xlim=c(0,max(as.numeric(rownames(wordList)))),
ylim=c(0,max(wordList)))
points(rownames(wordList),wordList[,1],col="#ff6347",pch=16)
points(rownames(wordList),wordList[,2],col="#ffd700",pch=16)
points(rownames(wordList),wordList[,3],col="#228b22",pch=16)
points(rownames(wordList),wordList[,4],col="#4169e1",pch=16)
View(dtm)
dtm@x
dtm@p
?create_dtm
data("movie_review")
N = 1000
it = itoken(movie_review$review[1:N], preprocess_function = tolower,
tokenizer = word_tokenizer)
v = create_vocabulary(it)
#remove very common and uncommon words
pruned_vocab = prune_vocabulary(v, term_count_min = 10,
doc_proportion_max = 0.5, doc_proportion_min = 0.001)
vectorizer = vocab_vectorizer(v)
it = itoken(movie_review$review[1:N], preprocess_function = tolower,
tokenizer = word_tokenizer)
dtm = create_dtm(it, vectorizer)
# get tf-idf matrix from bag-of-words matrix
dtm_tfidf = transformer_tfidf(dtm)
## Example of parallel mode
# set to number of cores on your machine
N_WORKERS = 1
doParallel::registerDoParallel(N_WORKERS)
splits = split_into(movie_review$review, N_WORKERS)
jobs = lapply(splits, itoken, tolower, word_tokenizer, chunks_number = 1)
vectorizer = hash_vectorizer()
dtm = create_dtm(jobs, vectorizer, type = 'dgTMatrix')
## End(Not run)
View(dtm)
View(dtm)
dtm@x
# read in the libraries we're going to use
library(tidyverse) # general utility & workflow functions
library(tidytext) # tidy implimentation of NLP methods
library(topicmodels) # for LDA topic modelling
library(tm) # general text mining functions, making document term matrixes
library(SnowballC) # for stemming
# function to get & plot the most informative terms by a specificed number
# of topics, using LDA
top_terms_by_topic_LDA <- function(input_text, # should be a columm from a dataframe
plot = T, # return a plot? TRUE by defult
number_of_topics = 4) # number of topics (4 by default)
{
# create a corpus (type of object expected by tm) and document term matrix
Corpus <- Corpus(VectorSource(input_text)) # make a corpus object
DTM <- DocumentTermMatrix(Corpus) # get the count of words/document
# remove any empty rows in our document term matrix (if there are any
# we'll get an error when we try to run our LDA)
unique_indexes <- unique(DTM$i) # get the index of each unique value
DTM <- DTM[unique_indexes,] # get a subset of only those indexes
# preform LDA & get the words/topic in a tidy text format
lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))
topics <- tidy(lda, matrix = "beta")
# get the top ten terms for each topic
top_terms <- topics  %>% # take the topics data frame and..
group_by(topic) %>% # treat each topic as a different group
top_n(10, beta) %>% # get the top 10 most informative words
ungroup() %>% # ungroup
arrange(topic, -beta) # arrange words in descending informativeness
# if the user asks for a plot (TRUE by default)
if(plot == T){
# plot the top ten terms for each topic in order
top_terms %>% # take the top terms
mutate(term = reorder(term, beta)) %>% # sort terms by beta value
ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
geom_col(show.legend = FALSE) + # as a bar plot
facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
labs(x = NULL, y = "Beta") + # no x label, change y label
coord_flip() # turn bars sideways
}else{
# if the user does not request a plot
# return a list of sorted terms instead
##return(top_terms)
return(lda)
}
}
# パッケージの読み込み
library(lda)
library(tm)
library(NLP)
# 分析データの読み込み
# 読み込む際に、全ての文字を小文字に変換
X1 <- tolower(scan("X1.txt", what = "char", sep = "\n", quiet = TRUE))
X2 <- tolower(scan("X2.txt", what = "char", sep = "\n", quiet = TRUE))
X3 <- tolower(scan("X3.txt", what = "char", sep = "\n", quiet = TRUE))
# 単語ベクトルに変換
X1.word.vector <- unlist(strsplit(X1, "([^-a-z0-9]+|--)"))
X2.word.vector <- unlist(strsplit(X2, "([^-a-z0-9]+|--)"))
X3.word.vector <- unlist(strsplit(X3, "([^-a-z0-9]+|--)"))
# ステミング処理
X1.stem <- stemDocument(X1.word.vector)
X2.stem <- stemDocument(X2.word.vector)
X3.stem <- stemDocument(X3.word.vector)
# ストップワードの除去
X1.sw <- removeWords(X1.stem, stopwords("english"))
X2.sw <- removeWords(X2.stem, stopwords("english"))
X3.sw <- removeWords(X3.stem, stopwords("english"))
# LDA用のデータ形式に変換
lex <- lexicalize(c(X1.sw, X2.sw, X3.sw))
# 推定するトピックの数
k <- 3
# LDA
# 以下の100, 0.1, 0.1は、それぞれ「繰り返し数」、「ディリクレ過程のハイパーパラメータα」、「ディリクレ過程のハイパーパラメータη」
result <- lda.collapsed.gibbs.sampler(lex$documents, k, lex$vocab, 100, 0.1, 0.1, compute.log.likelihood = TRUE)
# 結果の表示
summary(result)
# パッケージの読み込み
library(lda)
library(tm)
library(NLP)
# 分析データの読み込み
# 読み込む際に、全ての文字を小文字に変換
X1 <- tolower(scan("X1.txt", what = "char", sep = "\n", quiet = TRUE))
X2 <- tolower(scan("X2.txt", what = "char", sep = "\n", quiet = TRUE))
X3 <- tolower(scan("X3.txt", what = "char", sep = "\n", quiet = TRUE))
# 単語ベクトルに変換
X1.word.vector <- unlist(strsplit(X1, "([^-a-z0-9]+|--)"))
X2.word.vector <- unlist(strsplit(X2, "([^-a-z0-9]+|--)"))
X3.word.vector <- unlist(strsplit(X3, "([^-a-z0-9]+|--)"))
# ステミング処理
X1.stem <- stemDocument(X1.word.vector)
X2.stem <- stemDocument(X2.word.vector)
X3.stem <- stemDocument(X3.word.vector)
# ストップワードの除去
X1.sw <- removeWords(X1.stem, stopwords("english"))
X2.sw <- removeWords(X2.stem, stopwords("english"))
X3.sw <- removeWords(X3.stem, stopwords("english"))
# LDA用のデータ形式に変換
lex <- lexicalize(c(X1.sw, X2.sw, X3.sw))
# 推定するトピックの数
k <- 3
# LDA
# 以下の100, 0.1, 0.1は、それぞれ「繰り返し数」、「ディリクレ過程のハイパーパラメータα」、「ディリクレ過程のハイパーパラメータη」
result <- lda.collapsed.gibbs.sampler(lex$documents, k, lex$vocab, 100, 0.1, 0.1, compute.log.likelihood = TRUE)
# 結果の表示
summary(result)
setwd("x")
getwd()
setwd("x-men")
X3 <- tolower(scan("X3.txt", what = "char", sep = "\n", quiet = TRUE))
# 単語ベクトルに変換
X1.word.vector <- unlist(strsplit(X1, "([^-a-z0-9]+|--)"))
# パッケージの読み込み
library(lda)
library(tm)
library(NLP)
# 分析データの読み込み
# 読み込む際に、全ての文字を小文字に変換
X1 <- tolower(scan("x1.txt", what = "char", sep = "\n", quiet = TRUE))
X2 <- tolower(scan("x2.txt", what = "char", sep = "\n", quiet = TRUE))
X3 <- tolower(scan("X3.txt", what = "char", sep = "\n", quiet = TRUE))
# 単語ベクトルに変換
X1.word.vector <- unlist(strsplit(X1, "([^-a-z0-9]+|--)"))
X2.word.vector <- unlist(strsplit(X2, "([^-a-z0-9]+|--)"))
X3.word.vector <- unlist(strsplit(X3, "([^-a-z0-9]+|--)"))
# ステミング処理
X1.stem <- stemDocument(X1.word.vector)
X2.stem <- stemDocument(X2.word.vector)
X3.stem <- stemDocument(X3.word.vector)
# ストップワードの除去
X1.sw <- removeWords(X1.stem, stopwords("english"))
X2.sw <- removeWords(X2.stem, stopwords("english"))
X3.sw <- removeWords(X3.stem, stopwords("english"))
# LDA用のデータ形式に変換
lex <- lexicalize(c(X1.sw, X2.sw, X3.sw))
# 推定するトピックの数
k <- 3
# LDA
# 以下の100, 0.1, 0.1は、それぞれ「繰り返し数」、「ディリクレ過程のハイパーパラメータα」、「ディリクレ過程のハイパーパラメータη」
result <- lda.collapsed.gibbs.sampler(lex$documents, k, lex$vocab, 100, 0.1, 0.1, compute.log.likelihood = TRUE)
# 結果の表示
summary(result)
# パッケージの読み込み
library(lda)
library(tm)
library(NLP)
# 分析データの読み込み
# 読み込む際に、全ての文字を小文字に変換
X1 <- tolower(scan("x1.txt", what = "char", sep = "\n", quiet = TRUE))
X2 <- tolower(scan("x2.txt", what = "char", sep = "\n", quiet = TRUE))
X3 <- tolower(scan("x3.txt", what = "char", sep = "\n", quiet = TRUE))
# 単語ベクトルに変換
X1.word.vector <- unlist(strsplit(X1, "([^-a-z0-9]+|--)"))
X2.word.vector <- unlist(strsplit(X2, "([^-a-z0-9]+|--)"))
X3.word.vector <- unlist(strsplit(X3, "([^-a-z0-9]+|--)"))
# ステミング処理
X1.stem <- stemDocument(X1.word.vector)
X2.stem <- stemDocument(X2.word.vector)
X3.stem <- stemDocument(X3.word.vector)
# ストップワードの除去
X1.sw <- removeWords(X1.stem, stopwords("english"))
X2.sw <- removeWords(X2.stem, stopwords("english"))
X3.sw <- removeWords(X3.stem, stopwords("english"))
# LDA用のデータ形式に変換
lex <- lexicalize(c(X1.sw, X2.sw, X3.sw))
# 推定するトピックの数
k <- 3
# LDA
# 以下の100, 0.1, 0.1は、それぞれ「繰り返し数」、「ディリクレ過程のハイパーパラメータα」、「ディリクレ過程のハイパーパラメータη」
result <- lda.collapsed.gibbs.sampler(lex$documents, k, lex$vocab, 100, 0.1, 0.1, compute.log.likelihood = TRUE)
# 結果の表示
summary(result)
# 各トピックの上位10語を確認
top.words <- top.topic.words(result$topics, 10, by.score = TRUE)
print(top.words)
getwd()
list()
setwd("../wiki")
getwd()
# パッケージの読み込み
library(lda)
library(tm)
library(NLP)
# 分析データの読み込み
# 読み込む際に、全ての文字を小文字に変換
X1 <- tolower(scan("music.txt", what = "char", sep = "\n", quiet = TRUE))
X2 <- tolower(scan("sport.txt", what = "char", sep = "\n", quiet = TRUE))
X3 <- tolower(scan("theater.txt", what = "char", sep = "\n", quiet = TRUE))
# 単語ベクトルに変換
X1.word.vector <- unlist(strsplit(X1, "([^-a-z0-9]+|--)"))
X2.word.vector <- unlist(strsplit(X2, "([^-a-z0-9]+|--)"))
X3.word.vector <- unlist(strsplit(X3, "([^-a-z0-9]+|--)"))
# ステミング処理
X1.stem <- stemDocument(X1.word.vector)
X2.stem <- stemDocument(X2.word.vector)
X3.stem <- stemDocument(X3.word.vector)
# ストップワードの除去
X1.sw <- removeWords(X1.stem, stopwords("english"))
X2.sw <- removeWords(X2.stem, stopwords("english"))
X3.sw <- removeWords(X3.stem, stopwords("english"))
# LDA用のデータ形式に変換
lex <- lexicalize(c(X1.sw, X2.sw, X3.sw))
# 推定するトピックの数
k <- 3
# LDA
# 以下の100, 0.1, 0.1は、それぞれ「繰り返し数」、「ディリクレ過程のハイパーパラメータα」、「ディリクレ過程のハイパーパラメータη」
result <- lda.collapsed.gibbs.sampler(lex$documents, k, lex$vocab, 100, 0.1, 0.1, compute.log.likelihood = TRUE)
# 結果の表示
summary(result)
# 各トピックの上位10語を確認
top.words <- top.topic.words(result$topics, 10, by.score = TRUE)
print(top.words)
